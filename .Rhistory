noise <- torch_randn_like(model_output)
variance <- torch_sqrt(self$get_variance(t)) * noise
}
pred_prev_sample + variance
},
add_noise = function(x_start, x_noise, timesteps) {
s1 <- self$sqrt_alphas_cumprod[timesteps + 1L]
s2 <- self$sqrt_one_minus_alphas_cumprod[timesteps + 1L]
s1 <- s1$unsqueeze(-1)
s2 <- s2$unsqueeze(-1)
s1 * x_start + s2 * x_noise
}
)
scorch_2d_diffusion <- function(batch, noise_scheduler, ...) {
noise <- torch_randn(batch$input$shape)
timesteps <- torch_randint(0,  noise_scheduler$num_timesteps,
list(batch$input$shape[1])) |> torch_tensor(dtype = torch_long())
noisy <- noise_scheduler$add_noise(batch$input, noise, timesteps)
list(input = batch$input, output = batch$output,
noisy = noisy, timesteps = timesteps)
}
fitted_scorch_model <- compiled_scorch_model |>
fit_scorch(loss=nn_cross_entropy_loss, num_epochs = 2, verbose = T,
preprocess_fn = scorch_2d_diffusion)
fit_scorch = function(scorch_model,
loss = nn_mse_loss, loss_params = list(reduction = "mean"),
optim = optim_adam, optim_params = list(lr = 0.001),
num_epochs = 10, verbose = TRUE, preprocess_fn = NULL,
clip_grad_norm = F, max_norm = 1) {
loss_fn <- do.call(loss, loss_params)
optim_params <- append(list(
params = scorch_model$nn_model$parameters), optim_params)
optim_fn <- do.call(optim, optim_params)
length_dl <- length(scorch_model$dl)
for (epoch in 1:num_epochs) {
total_loss = 0
coro::loop(for (batch in scorch_model$dl) {
if (!is.null(preprocess_fn)) {                  ## Note: preprocess_fun must output at minimum a list with items `input` and `output`
preprocessed <- preprocess_fn(batch, ...)
for (i in 1:length(preprocessed)) {
assign(names(preprocessed)[i], preprocessed[[i]])
}
} else {
input <- batch$input
output <- batch$output
}
optim_fn$zero_grad()
## Diffusion
if (exists("noisy") && exists("timesteps")) {
pred <- scorch_model$nn_model(noisy, timesteps)
} else {
pred <- scorch_model$nn_model(input)
}
loss <- loss_fn(pred, output)
loss$backward()
if (clip_grad_norm) {
nn_utils_clip_grad_norm_(model$parameters, max_norm)
}
optim_fn$step()
total_loss <- total_loss + loss$item()
})
if(verbose){
cat(glue::glue("Epoch {crayon::red(epoch)}, ",
"Loss: {crayon::red(total_loss/length_dl)} \n\n"))
}
}
return(scorch_model$nn_model)
}
fitted_scorch_model <- compiled_scorch_model |>
fit_scorch(loss=nn_cross_entropy_loss, num_epochs = 2, verbose = T,
preprocess_fn = scorch_2d_diffusion, clip_grad_norm = T)
fit_scorch = function(scorch_model,
loss = nn_mse_loss, loss_params = list(reduction = "mean"),
optim = optim_adam, optim_params = list(lr = 0.001),
num_epochs = 10, verbose = TRUE, preprocess_fn = NULL,
clip_grad_norm = F, max_norm = 1, ...) {
loss_fn <- do.call(loss, loss_params)
optim_params <- append(list(
params = scorch_model$nn_model$parameters), optim_params)
optim_fn <- do.call(optim, optim_params)
length_dl <- length(scorch_model$dl)
for (epoch in 1:num_epochs) {
total_loss = 0
coro::loop(for (batch in scorch_model$dl) {
if (!is.null(preprocess_fn)) {                  ## Note: preprocess_fun must output at minimum a list with items `input` and `output`
preprocessed <- preprocess_fn(batch, ...)
for (i in 1:length(preprocessed)) {
assign(names(preprocessed)[i], preprocessed[[i]])
}
} else {
input <- batch$input
output <- batch$output
}
optim_fn$zero_grad()
## Diffusion
if (exists("noisy") && exists("timesteps")) {
pred <- scorch_model$nn_model(noisy, timesteps)
} else {
pred <- scorch_model$nn_model(input)
}
loss <- loss_fn(pred, output)
loss$backward()
if (clip_grad_norm) {
nn_utils_clip_grad_norm_(model$parameters, max_norm)
}
optim_fn$step()
total_loss <- total_loss + loss$item()
})
if(verbose){
cat(glue::glue("Epoch {crayon::red(epoch)}, ",
"Loss: {crayon::red(total_loss/length_dl)} \n\n"))
}
}
return(scorch_model$nn_model)
}
noise_scheduler <- NoiseScheduler$new(
num_timesteps = 50L,
beta_schedule = "linear"
)
fitted_scorch_model <- compiled_scorch_model |>
fit_scorch(loss=nn_cross_entropy_loss, num_epochs = 2, verbose = T,
preprocess_fn = scorch_2d_diffusion, clip_grad_norm = T,
noise_scheduler = noise_scheduler)
batch
dl$batch_sampler
coro::loop(for (batch in compiled_scorch_model$dl) { batch$input }
)
coro::loop(for (batch in compiled_scorch_model$dl) { print(batch$input) })
### Replace with scorcher dataloader!!!
input <- output <- dino_dataset()
### Replace with scorcher dataloader!!!
input <- output <- dino_dataset()
scorch_create_dataloader(input, output, batch_size = 500)
### Replace with scorcher dataloader!!!
input <- output <- dino_dataset()$data
scorch_create_dataloader(input, output, batch_size = 500)
scorch_create_dataloader(input, output, batch_size = 32)
scorch_model <- dl |>
initiate_scorch() |>
scorch_function(PositionalEmbedding(size = 128, type = "sinusoidal")$forward) |>
scorch_function(PositionalEmbedding(size = 128, type = "sinusoidal", scale = 25.0)$forward) |>
scorch_function(PositionalEmbedding(size = 128, type = "sinusoidal", scale = 25.0)$forward) |>
scorch_layer(nn_linear(128 * 3, 128)) |>
scorch_layer(nn_gelu()) |>
scorch_layer(nn_linear(128, 128)) |>
scorch_layer(nn_gelu()) |>
scorch_layer(nn_linear(128, 128)) |>
scorch_layer(nn_gelu()) |>
scorch_layer(nn_linear(128, 128)) |>
scorch_layer(nn_gelu()) |>
scorch_layer(nn_linear(128, 2))
compiled_scorch_model <- scorch_model |>
compile_scorch()
noise_scheduler <- NoiseScheduler$new(
num_timesteps = 50L,
beta_schedule = "linear"
)
fitted_scorch_model <- compiled_scorch_model |>
fit_scorch(loss=nn_cross_entropy_loss, num_epochs = 2, verbose = T,
preprocess_fn = scorch_2d_diffusion, clip_grad_norm = T,
noise_scheduler = noise_scheduler)
batch <- dataloader_next(dl)
dl
batch <- dataloader_next(dl)
for (batch in enumerate(dl)) {
print(batch)
break  # Exit after the first batch for inspection
}
batch$run()
dim(batch)
batch$input
batch$input$shape
batch$input()
batch$input$shape()
batch[1]
batch[[1]]
test <- dataset$data
test
test$shape
for (batch in enumerate(compiled_scorch_model$dl)) {
print(batch)
break  # Exit after the first batch for inspection
}
names(batch)
dataset <- dino_dataset()
input <- output <- dataset$data
dl <-scorch_create_dataloader(input, output, batch_size = 32)
dl
iter <- dataloader_make_iter(dl)
# Fetch the next batch
batch <- iter$.next()
# Inspect the contents of the batch
print(batch)
batch$input$shape
scorch_model <- dl |>
initiate_scorch() |>
scorch_function(PositionalEmbedding(size = 128, type = "sinusoidal")$forward) |>
scorch_function(PositionalEmbedding(size = 128, type = "sinusoidal", scale = 25.0)$forward) |>
scorch_function(PositionalEmbedding(size = 128, type = "sinusoidal", scale = 25.0)$forward) |>
scorch_layer(nn_linear(128 * 3, 128)) |>
scorch_layer(nn_gelu()) |>
scorch_layer(nn_linear(128, 128)) |>
scorch_layer(nn_gelu()) |>
scorch_layer(nn_linear(128, 128)) |>
scorch_layer(nn_gelu()) |>
scorch_layer(nn_linear(128, 128)) |>
scorch_layer(nn_gelu()) |>
scorch_layer(nn_linear(128, 2))
compiled_scorch_model <- scorch_model |>
compile_scorch()
fitted_scorch_model <- compiled_scorch_model |>
fit_scorch(loss=nn_cross_entropy_loss, num_epochs = 2, verbose = F)
SinusoidalEmbedding <- nn_module(
initialize = function(size, scale = 1.0) {
self$size <- size
self$scale <- scale
},
forward = function(x) {
x <- x * self$scale
half_size <- self$size %/% 2
emb <- torch_log(torch_tensor(10000.0)) / (half_size - 1)
emb <- torch_exp(-emb * torch_arange(1, half_size))
emb <- x$unsqueeze(-1) * emb$unsqueeze(1)
emb <- torch_cat(list(torch_sin(emb), torch_cos(emb)), dim = -1)
emb
}
)
LinearEmbedding <- nn_module(
initialize = function(size, scale = 1.0) {
self$size <- size
self$scale <- scale
},
forward = function(x) {
x <- x / self$size * self$scale
x$unsqueeze(-1)
}
)
LearnableEmbedding <- nn_module(
initialize = function(size) {
self$size <- size
self$linear <- nn_linear(1, size)
},
forward = function(x) {
self$linear(x$unsqueeze(-1) / self$size)
}
)
IdentityEmbedding <- nn_module(
initialize = function() {},
forward = function(x) {
x$unsqueeze(-1)
}
)
ZeroEmbedding <- nn_module(
initialize = function() {},
forward = function(x) {
x$unsqueeze(-1) * 0
}
)
PositionalEmbedding <- nn_module(
initialize = function(size, type, ...) {
if (type == "sinusoidal") {
self$layer <- SinusoidalEmbedding(size, ...)
} else if (type == "linear") {
self$layer <- LinearEmbedding(size, ...)
} else if (type == "learnable") {
self$layer <- LearnableEmbedding(size)
} else if (type == "zero") {
self$layer <- ZeroEmbedding()
} else if (type == "identity") {
self$layer <- IdentityEmbedding()
} else {
stop("Unknown positional embedding type: ", type)
}
},
forward = function(x) {
self$layer(x)
}
)
noise_scheduler <- NoiseScheduler$new(
num_timesteps = 50L,
beta_schedule = "linear"
)
fitted_scorch_model <- compiled_scorch_model |>
fit_scorch(loss=nn_cross_entropy_loss, num_epochs = 2, verbose = T,
preprocess_fn = scorch_2d_diffusion, clip_grad_norm = T,
noise_scheduler = noise_scheduler)
scorch_2d_diffusion_forward <- function(input, timestep, size = 128, type = "sinusoidal", scale = 1, ...) {
x1_emb <- PositionalEmbedding(size, type, scale)$forward(input[,1])
x2_emb <- PositionalEmbedding(size, type, scale)$forward(input[,2])
t_emb  <- PositionalEmbedding(size, type)$forward(timestep)
input <- torch_cat(list(x1_emb, x2_emb, t_emb), dim = -1)
input
}
compile_scorch <- function(sm, preprocess_fn = NULL, ...) {
model <- nn_module(
initialize = function(sm) {
n_layer = length(sm$scorch_architecture) / 2
layer_types = sm$scorch_architecture[2 * (1:n_layer)] |> unlist()
layer_index = which(layer_types == "layer") * 2 - 1
func_index = which(layer_types == "function") * 2 - 1
modules = nn_module_list(sm$scorch_architecture[layer_index])
self$modules = modules
self$functions = sm$scorch_architecture[func_index]
},
forward = function(input, ...) {
if (!is.null(preprocess_fn)) {
input <- preprocess_fn(input, ...)
}
n_layer = length(sm$scorch_architecture) / 2
layer_types = sm$scorch_architecture[2 * (1:n_layer)] |> unlist()
layer_index = which(layer_types == "layer")
output = input
i_layer = i_function = 1
for(i in 1:n_layer) {
if(i %in% layer_index) {
output = self$modules[[i_layer]](output)
i_layer = i_layer + 1
} else {
output = self$functions[[i_function]](output)
i_function = i_function + 1
}
}
return(output)
}
)
return(list(nn_model = sm |> model(), dl = sm$dl))
}
compiled_scorch_model <- scorch_model |>
compile_scorch(preprocess_fn = scorch_2d_diffusion_forward(scale = 1))
compiled_scorch_model <- scorch_model |>
compile_scorch(preprocess_fn = scorch_2d_diffusion_forward(scale = 25))
noise_scheduler <- NoiseScheduler$new(
num_timesteps = 50L,
beta_schedule = "linear"
)
scorch_2d_diffusion_train <- function(batch, noise_scheduler, ...) {
noise <- torch_randn(batch$input$shape)
timesteps <- torch_randint(0,  noise_scheduler$num_timesteps,
list(batch$input$shape[1])) |> torch_tensor(dtype = torch_long())
noisy <- noise_scheduler$add_noise(batch$input, noise, timesteps)
list(input = batch$input, output = batch$output,
noisy = noisy, timesteps = timesteps)
}
compiled_scorch_model <- scorch_model |>
compile_scorch(preprocess_fn = scorch_2d_diffusion_forward, scale = 25)
noise_scheduler <- NoiseScheduler$new(
num_timesteps = 50L,
beta_schedule = "linear"
)
fitted_scorch_model <- compiled_scorch_model |>
fit_scorch(loss=nn_cross_entropy_loss, num_epochs = 2, verbose = T,
preprocess_fn = scorch_2d_diffusion_train, clip_grad_norm = T,
noise_scheduler = noise_scheduler)
fitted_scorch_model$eval()
output <- fitted_scorch_model(output)
compile_scorch <- function(sm, init_fn = NULL, forward_fn = NULL) {
model <- nn_module(
initialize = function(sm) {
n_layer = length(sm$scorch_architecture) / 2
layer_types = sm$scorch_architecture[2 * (1:n_layer)] |> unlist()
layer_index = which(layer_types == "layer") * 2 - 1
func_index = which(layer_types == "function") * 2 - 1
modules = nn_module_list(sm$scorch_architecture[layer_index])
self$modules = modules
self$functions = sm$scorch_architecture[func_index]
if (!is.null(init_fn)) {
init_fn(self)
}
},
forward = function(input, ...) {
if (!is.null(forward_fn)) {
input <- forward_fn(self, input, ...)
}
n_layer = length(sm$scorch_architecture) / 2
layer_types = sm$scorch_architecture[2 * (1:n_layer)] |> unlist()
layer_index = which(layer_types == "layer")
output = input
i_layer = i_function = 1
for(i in 1:n_layer) {
if(i %in% layer_index) {
output = self$modules[[i_layer]](output)
i_layer = i_layer + 1
} else {
output = self$functions[[i_function]](output)
i_function. = i_function + 1
}
}
return(output)
}
)
return(list(nn_model = sm |> model(), dl = sm$dl))
}
compile_scorch <- function(sm, init_fn = NULL, forward_fn = NULL, ...) {
model <- nn_module(
initialize = function(sm) {
n_layer = length(sm$scorch_architecture) / 2
layer_types = sm$scorch_architecture[2 * (1:n_layer)] |> unlist()
layer_index = which(layer_types == "layer") * 2 - 1
func_index = which(layer_types == "function") * 2 - 1
modules = nn_module_list(sm$scorch_architecture[layer_index])
self$modules = modules
self$functions = sm$scorch_architecture[func_index]
if (!is.null(init_fn)) {
init_fn(self)
}
},
forward = function(input, ...) {
if (!is.null(forward_fn)) {
input <- forward_fn(self, input, ...)
}
n_layer = length(sm$scorch_architecture) / 2
layer_types = sm$scorch_architecture[2 * (1:n_layer)] |> unlist()
layer_index = which(layer_types == "layer")
output = input
i_layer = i_function = 1
for(i in 1:n_layer) {
if(i %in% layer_index) {
output = self$modules[[i_layer]](output)
i_layer = i_layer + 1
} else {
output = self$functions[[i_function]](output)
i_function. = i_function + 1
}
}
return(output)
}
)
return(list(nn_model = sm |> model(), dl = sm$dl))
}
scorch_2d_diffusion_init <- function(model, emb_size, time_emb, imput_emb, scale = 1) {
model$self$time_mlp <- PositionalEmbedding(emb_size, type = time_emb)
model$self$input_mlp1 <- PositionalEmbedding(emb_size, type = input_emb, scale = scale)
model$self$input_mlp2 <- PositionalEmbedding(emb_size, type = input_emb, scale = scale)
}
scorch_2d_diffusion_forward <- function(model, input, timestep, size = 128, type = "sinusoidal", scale = 1, ...) {
x1_emb <- model$self$input_mlp1(input[, 1])
x2_emb <- model$self$input_mlp2(input[, 2])
t_emb  <- model$self$time_mlp(timestep)
input  <- torch_cat(list(x1_emb, x2_emb, t_emb), dim = -1)
input
}
compile_scorch <- function(sm, init_fn = NULL, forward_fn = NULL, ...) {
model <- nn_module(
initialize = function(sm) {
n_layer = length(sm$scorch_architecture) / 2
layer_types = sm$scorch_architecture[2 * (1:n_layer)] |> unlist()
layer_index = which(layer_types == "layer") * 2 - 1
func_index = which(layer_types == "function") * 2 - 1
modules = nn_module_list(sm$scorch_architecture[layer_index])
self$modules = modules
self$functions = sm$scorch_architecture[func_index]
if (!is.null(init_fn)) {
init_fn(self)
}
},
forward = function(input, ...) {
if (!is.null(forward_fn)) {
input <- forward_fn(self, input, ...)
}
n_layer = length(sm$scorch_architecture) / 2
layer_types = sm$scorch_architecture[2 * (1:n_layer)] |> unlist()
layer_index = which(layer_types == "layer")
output = input
i_layer = i_function = 1
for(i in 1:n_layer) {
if(i %in% layer_index) {
output = self$modules[[i_layer]](output)
i_layer = i_layer + 1
} else {
output = self$functions[[i_function]](output)
i_function. = i_function + 1
}
}
return(output)
}
)
return(list(nn_model = sm |> model(), dl = sm$dl))
}
compiled_scorch_model <- scorch_model |>
compile_scorch(init_fn = scorch_2d_diffusion_init, forward_fn = scorch_2d_diffusion_forward, scale = 25)
scorch_2d_diffusion_init <- function(model, emb_size = 128, time_emb = "sinusoidal", imput_emb = "sinusoidal", scale = 1) {
model$self$time_mlp <- PositionalEmbedding(emb_size, type = time_emb)
model$self$input_mlp1 <- PositionalEmbedding(emb_size, type = input_emb, scale = scale)
model$self$input_mlp2 <- PositionalEmbedding(emb_size, type = input_emb, scale = scale)
}
scorch_2d_diffusion_forward <- function(model, input, timestep) {
x1_emb <- model$self$input_mlp1(input[, 1])
x2_emb <- model$self$input_mlp2(input[, 2])
t_emb  <- model$self$time_mlp(timestep)
input  <- torch_cat(list(x1_emb, x2_emb, t_emb), dim = -1)
input
}
compiled_scorch_model <- scorch_model |>
compile_scorch(init_fn = scorch_2d_diffusion_init, forward_fn = scorch_2d_diffusion_forward, scale = 25)
scorch_2d_diffusion_init <- function(model, emb_size = 128, time_emb = "sinusoidal", input_emb = "sinusoidal", scale = 1) {
model$self$time_mlp <- PositionalEmbedding(emb_size, type = time_emb)
model$self$input_mlp1 <- PositionalEmbedding(emb_size, type = input_emb, scale = scale)
model$self$input_mlp2 <- PositionalEmbedding(emb_size, type = input_emb, scale = scale)
}
scorch_2d_diffusion_forward <- function(model, input, timestep) {
x1_emb <- model$self$input_mlp1(input[, 1])
x2_emb <- model$self$input_mlp2(input[, 2])
t_emb  <- model$self$time_mlp(timestep)
input  <- torch_cat(list(x1_emb, x2_emb, t_emb), dim = -1)
input
}
compiled_scorch_model <- scorch_model |>
compile_scorch(init_fn = scorch_2d_diffusion_init, forward_fn = scorch_2d_diffusion_forward, scale = 25)
fitted_scorch_model <- compiled_scorch_model |>
fit_scorch(loss=nn_cross_entropy_loss, num_epochs = 2, verbose = T,
preprocess_fn = scorch_2d_diffusion_train, clip_grad_norm = T,
noise_scheduler = noise_scheduler)
